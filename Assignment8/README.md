# OneCycleLR on Resnet18 with CIFAR10

### Resnet Model

     ----------------------------------------------------------------
            Layer (type)               Output Shape         Param #
    ================================================================
                Conv2d-1           [-1, 64, 32, 32]           1,792
           BatchNorm2d-2           [-1, 64, 32, 32]             128
                  ReLU-3           [-1, 64, 32, 32]               0
                Conv2d-4          [-1, 128, 32, 32]          73,856
             MaxPool2d-5          [-1, 128, 16, 16]               0
           BatchNorm2d-6          [-1, 128, 16, 16]             256
                  ReLU-7          [-1, 128, 16, 16]               0
                Conv2d-8          [-1, 128, 16, 16]         147,584
           BatchNorm2d-9          [-1, 128, 16, 16]             256
                 ReLU-10          [-1, 128, 16, 16]               0
               Conv2d-11          [-1, 128, 16, 16]         147,584
          BatchNorm2d-12          [-1, 128, 16, 16]             256
                 ReLU-13          [-1, 128, 16, 16]               0
          ResNetBlock-14          [-1, 128, 16, 16]               0
               Conv2d-15          [-1, 256, 16, 16]         295,168
            MaxPool2d-16            [-1, 256, 8, 8]               0
          BatchNorm2d-17            [-1, 256, 8, 8]             512
                 ReLU-18            [-1, 256, 8, 8]               0
               Conv2d-19            [-1, 512, 8, 8]       1,180,160
            MaxPool2d-20            [-1, 512, 4, 4]               0
          BatchNorm2d-21            [-1, 512, 4, 4]           1,024
                 ReLU-22            [-1, 512, 4, 4]               0
               Conv2d-23            [-1, 512, 4, 4]       2,359,808
          BatchNorm2d-24            [-1, 512, 4, 4]           1,024
                 ReLU-25            [-1, 512, 4, 4]               0
               Conv2d-26            [-1, 512, 4, 4]       2,359,808
          BatchNorm2d-27            [-1, 512, 4, 4]           1,024
                 ReLU-28            [-1, 512, 4, 4]               0
          ResNetBlock-29            [-1, 512, 4, 4]               0
            MaxPool2d-30            [-1, 512, 1, 1]               0
               Linear-31                   [-1, 10]           5,130
               ResNet-32                   [-1, 10]               0
    ================================================================
    Total params: 6,575,370
    Trainable params: 6,575,370
    Non-trainable params: 0
    ----------------------------------------------------------------
    Input size (MB): 0.01
    Forward/backward pass size (MB): 6.75
    Params size (MB): 25.08
    Estimated Total Size (MB): 31.85
    ----------------------------------------------------------------
 
 ### Image Augmentation
 
To introduce reguralization and prevent overfitting of the model, the following transformations are applied to the training dataset.

     main.prepare_trainloader([
       A.RandomCrop(32, 32),
       A.HorizontalFlip(p=0.5),
       A.Cutout(),
       A.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)),
       A.pytorch.transforms.ToTensorV2()
     ])
 
 
 ### Finding the Maximum LR
 To perform OneCycleLR, we need to first find out the maximum LR value for the cycle. We do this using a Range Test. The model is trained for 1 epoch with the Learning Rate increasing exponentially with each batch. The values of LRs and their corresponding losses are noted and plotted in a graph.
 
    Learning Rates: [1.029882607207437e-05, 1.060658184628388e-05, 1.0923534165409913e-05, 1.1249957846191874e-05, 1.1586135917609852e-05, 1.1932359866287762e-05, 1.2288929889229824e-05, 1.2656155154109411e-05, 1.3034354067336042e-05, 1.3423854550132903e-05, 1.382499432286429e-05, 1.423812119785949e-05, 1.4663593380987007e-05, 1.5101779782240613e-05, 1.555306033560652e-05, 1.601782632848902e-05, 1.64964807409802e-05, 1.698943859526796e-05, 1.749712731548522e-05, 1.801998709831238e-05, 1.8558471294654332e-05, 1.9113046802722984e-05, 1.968419447286611e-05, 2.0272409524493572e-05, 2.0878201975462318e-05, 2.1502097084292593e-05, 2.214463580559868e-05, 2.2806375259129134e-05, 2.3487889212823095e-05, 2.4189768580301685e-05, 2.491262193322564e-05, 2.5657076028963604e-05, 2.6423776354028466e-05, 2.721338768375306e-05, 2.8026594658690357e-05, 2.886410237823805e-05, 2.9726637012002185e-05, 3.0614946429429905e-05, 3.152980084825729e-05, 3.2471993502334466e-05, 3.344234132940718e-05, 3.4441685679450885e-05, 3.547089304417192e-05, 3.653085580830792e-05, 3.76224930233791e-05, 3.874675120456128e-05, 3.990460515137147e-05, 4.109705879287776e-05, 4.232514605816627e-05, 4.3589931772819856e-05, 4.489251258218601e-05, 4.6234017902234394e-05, 4.7615610898828476e-05, 4.9038489496260325e-05, 5.050388741592309e-05, 5.201307524602173e-05, 5.3567361543249464e-05, 5.5168093967385154e-05, 5.68166604497955e-05, 5.851449039685506e-05, 6.026305592932762e-05, 6.206387315878352e-05, 6.391850350215964e-05, 6.582855503560186e-05, 6.77956838887639e-05, 6.982159568077139e-05, 7.190804699909636e-05, 7.405684692262427e-05, 7.626985859023433e-05, 7.854900081625308e-05, 8.08962497541818e-05, 8.331364061014075e-05, 8.580326940751514e-05, 8.83672948043338e-05, 9.100793996495551e-05, 9.372749448768628e-05, 9.652831638999902e-05, 9.941283415307656e-05, 0.00010238354882745101, 0.00010544303620156518, 0.00010859394903513612, 0.00011183901935925751, 0.00011518106084523513, 0.00011862297124420921, 0.000122167734899679, 0.0001258184253351084, 0.00012957820791885567, 0.00013345034260873844, 0.00013743818677861325, 0.0001415451981294209, 0.00014577493768722124, 0.00015013107289081707, 0.00015461738077164443, 0.0001592377512286862, 0.00016399619040124858, 0.00016889682414252517, 0.0001739439015969598, 0.00017914179888451083, 0.0001844950228950103, 0.00019000821519590897, 0.00019568615605679448, 0.0002015337685941729, 0.00020755612304010706, 0.00021375844113841305, 0.00022014610067222628, 0.00022672464012686328, 0.00023349976349202185, 0.00024047734520748338, 0.0002476634352566058, 0.00025506426441202347, 0.0002626862496381018, 0.0002705359996548319, 0.00027862032066798857, 0.0002869462222705202, 0.00029552092352028804, 0.0003043518591994238, 0.0003134466862607334, 0.0003228132904667356, 0.00033245979322709334, 0.00034239455864036426, 0.000352626200746178, 0.0003631635909941268, 0.00037401586593584663, 0.0003851924351469569, 0.00039670298938572956, 0.0004085575089955594, 0.00042076627255852256, 0.00043333986580752624, 0.00044628919080477605, 0.00045962547539452, 0.000473360282938266, 0.00048750552234091136, 0.0005020734583764812, 0.0005170767223224251, 0.000532528322911695, 0.0005484416576121004, 0.0005648305242427184, 0.0005817091329374342, 0.0005990921184659823, 0.0006169945529231726, 0.0006354319587973039, 0.000654420322429096, 0.0006739761078728088, 0.0006941162711715693, 0.0007148582750592801, 0.0007362201041018624, 0.0007582202802909568, 0.0007808778791036043, 0.0008042125460418337, 0.0008282445136664947, 0.0008529946191401051, 0.000878484322293926, 0.000904735724234927, 0.0009317715865087752, 0.0009596153508354675, 0.0009882911594347105, 0.0010178238759586803, 0.0010482391070503047, 0.0010795632245457635, 0.0011118233883404584, 0.0011450475699382782, 0.0011792645767045738, 0.001214504076843881, 0.0012507966251240376, 0.001288173689369007, 0.0013266676777433759, 0.0013663119668521836, 0.0014071409306804482, 0.0014491899703974794, 0.0014924955450518244, 0.0015370952031834575, 0.0015830276153806246, 0.0016303326078095691, 0.001679051196746219, 0.0017292256241397631, 0.0017808993942389666, 0.0018341173113129723, 0.001888925518499298, 0.0019453715378127167, 0.002003504311349702, 0.0020633742437241717, 0.002125033245771323, 0.0021885347795574527, 0.0022539339047347824, 0.0023212873262814966, 0.002390653443668368, 0.0024620924014946166, 0.002535666141636895, 0.002611438456956628, 0.0026894750466122577, 0.002769843573024375, 0.0028526137205431065, 0.0029378572558686417, 0.0030256480902772828, 0.0031160623437069706, 0.003209178410757852, 0.003305077028665115, 0.003403841347303038, 0.0035055570012809275, 0.0036103121841934856, 0.0037181977250899637, 0.0038293071672284127, 0.003943736849183322, 0.004061585988376962, 0.0041829567671068605, 0.004307954421144006, 0.004436687330978593, 0.004569267115792438, 0.004705808730239522, 0.004846430564118598, 0.004991254545024271, 0.005140406244065566, 0.0052940149847436335, 0.005452213955083014, 0.005615140323113665, 0.005782935355803912, 0.0059557445415474, 0.006133717716310297, 0.006317009193548095, 0.006505777898004661, 0.006700187503509559, 0.006900406574893113, 0.00710660871414226, 0.00731897271092392, 0.0075376826976064105, 0.0077629283089132774, 0.007994904846348025, 0.008233813447532276, 0.008479861260604196, 0.008733261623828393, 0.00899423425057304, 0.00926300541981459, 0.00953980817233527, 0.009824882512783462, 0.010118475617772186, 0.0104208420501961, 0.01073224397995285, 0.011052951411260163, 0.011383242416765737, 0.011723403378652983, 0.01207372923695161, 0.012434523745268381, 0.012806099734159785, 0.013188779382374945, 0.013582894496203998, 0.01398878679717412, 0.014406808218342655, 0.014837321209444261, 0.015280699051156659, 0.01573732617875743, 0.01620759851545255, 0.016691923815665657, 0.017190722018585656, 0.01770442561227929, 0.018233480008684317, 0.018778343929808487, 0.01933948980546911, 0.019917404182918177, 0.020512588148708086, 0.021125557763163855, 0.021756844507838498, 0.022406995746339517, 0.02307657519892609, 0.023766163431288484, 0.02447635835793343, 0.02520777576061202, 0.02596104982223954, 0.026736833676770228, 0.027535799975503723, 0.028358641470314253, 0.029206071614308188, 0.030078825180430833, 0.03097765889855881, 0.03190335211163041, 0.03285670745138282, 0.033838551534282155, 0.03484973567824972, 0.035891136640805864, 0.03696365737927151, 0.03806822783368656, 0.039205805733123836, 0.04037737742609785, 0.041583958735788365, 0.04282659584082019, 0.04410636618236308, 0.045424379398338016, 0.04678177828554014, 0.04817973979051234, 0.04961947603002874, 0.05110223534207293, 0.052629303368222094, 0.05420200416837571, 0.05582170136879514, 0.0574897993444497, 0.05920774443669425, 0.0609770262073343, 0.06279917873016566, 0.06467578192110883, 0.06660846290809118, 0.0685988974418648, 0.0706488113489833, 0.07275998202819729, 0.07493423999156608, 0.07717347045162186, 0.07947961495596242, 0.08185467307068978, 0.08430070411415437]
    Losses: [2.894618511199951, 2.712022304534912, 2.947608470916748, 2.6423985958099365, 2.915708541870117, 2.7345314025878906, 2.734654188156128, 2.7197067737579346, 2.668097734451294, 2.736386299133301, 2.7681939601898193, 2.8127734661102295, 2.69694185256958, 2.8980095386505127, 2.575634002685547, 2.6239984035491943, 2.580177068710327, 2.6407113075256348, 2.673527956008911, 2.609095335006714, 2.4815239906311035, 2.573655605316162, 2.5344650745391846, 2.5476491451263428, 2.5816574096679688, 2.500006914138794, 2.4219417572021484, 2.651869535446167, 2.545468330383301, 2.540963649749756, 2.3951098918914795, 2.4885125160217285, 2.4076831340789795, 2.4591846466064453, 2.422346353530884, 2.4070608615875244, 2.4297525882720947, 2.33526873588562, 2.448735237121582, 2.350501298904419, 2.3275554180145264, 2.364442825317383, 2.3005576133728027, 2.2764031887054443, 2.366140127182007, 2.421189785003662, 2.3573341369628906, 2.354302406311035, 2.332702875137329, 2.2947001457214355, 2.3260085582733154, 2.4084489345550537, 2.3480019569396973, 2.2992045879364014, 2.2104947566986084, 2.359999179840088, 2.2775936126708984, 2.2381505966186523, 2.209744930267334, 2.2547903060913086, 2.251910448074341, 2.2475063800811768, 2.253159523010254, 2.1517317295074463, 2.226724624633789, 2.2728893756866455, 2.2963199615478516, 2.269580841064453, 2.281913995742798, 2.21240496635437, 2.3023698329925537, 2.2965171337127686, 2.2799031734466553, 2.206176280975342, 2.1572296619415283, 2.2579681873321533, 2.2039995193481445, 2.1192941665649414, 2.12160587310791, 2.1413683891296387, 2.161168098449707, 2.2167587280273438, 2.1548855304718018, 2.159331798553467, 2.1631882190704346, 2.169299840927124, 2.0934841632843018, 2.187237024307251, 2.1226589679718018, 2.1685874462127686, 2.139780282974243, 2.164044141769409, 2.0411458015441895, 2.1554934978485107, 2.2065773010253906, 2.099684476852417, 2.0560832023620605, 2.0819389820098877, 2.242231845855713, 2.0679397583007812, 2.087146759033203, 1.9951930046081543, 2.118849039077759, 2.092085123062134, 2.071892499923706, 2.053189516067505, 2.1095263957977295, 2.127708911895752, 1.9419618844985962, 2.0734050273895264, 1.9908108711242676, 1.996450662612915, 2.194211006164551, 2.0314953327178955, 1.999168038368225, 1.9653593301773071, 1.973600149154663, 1.984283447265625, 1.9943232536315918, 2.08971905708313, 1.9710664749145508, 2.0318803787231445, 2.0014357566833496, 1.999074935913086, 1.9188134670257568, 2.000286340713501, 1.9956719875335693, 1.9530258178710938, 1.9690358638763428, 1.9637809991836548, 1.8817236423492432, 1.8958613872528076, 1.896156907081604, 1.9129687547683716, 1.9970293045043945, 1.847862720489502, 1.9067025184631348, 1.8433181047439575, 1.8714386224746704, 1.8658465147018433, 1.9713678359985352, 1.9733219146728516, 1.8976497650146484, 1.9058115482330322, 1.8798153400421143, 1.8744945526123047, 1.831107258796692, 1.762826919555664, 1.8764393329620361, 1.8264530897140503, 1.8206422328948975, 1.9482295513153076, 1.9526487588882446, 1.7570431232452393, 1.782891869544983, 1.8362573385238647, 1.9721064567565918, 1.7190841436386108, 1.89007568359375, 1.809605360031128, 1.876194715499878, 1.7696146965026855, 1.7902003526687622, 1.821941614151001, 1.6252325773239136, 1.7422221899032593, 1.8590927124023438, 1.9557029008865356, 1.7065842151641846, 1.6084219217300415, 1.7415670156478882, 1.7615091800689697, 1.9182604551315308, 1.7640904188156128, 1.7430075407028198, 1.8058881759643555, 1.7022062540054321, 1.6281603574752808, 1.654253602027893, 1.6505235433578491, 1.8488647937774658, 1.5841221809387207, 1.7762279510498047, 1.6328237056732178, 1.7046151161193848, 1.6039671897888184, 1.8148949146270752, 1.7338507175445557, 1.7109037637710571, 1.7056304216384888, 1.80364191532135, 1.5487629175186157, 1.794766902923584, 1.5780131816864014, 1.8350415229797363, 1.8544468879699707, 1.9143551588058472, 1.7256543636322021, 1.8897335529327393, 1.5576972961425781, 1.7669241428375244, 1.7651731967926025, 1.6154062747955322, 1.9237629175186157, 1.6282814741134644, 1.876823902130127, 1.6494140625, 1.811933159828186, 1.650296926498413, 1.7843347787857056, 1.9803736209869385, 1.6916754245758057, 2.0697133541107178, 2.1122350692749023, 1.597585678100586, 1.6435835361480713, 1.8079869747161865, 1.8383773565292358, 2.03560471534729, 1.846536636352539, 1.9801993370056152, 1.8390394449234009, 1.5909368991851807, 1.8259382247924805, 1.7388821840286255, 1.7555495500564575, 1.8300461769104004, 1.7988158464431763, 1.6385059356689453, 2.016367197036743, 2.2912180423736572, 2.0371947288513184, 2.7353293895721436, 2.5633108615875244, 3.4590697288513184, 3.1233768463134766, 2.5760421752929688, 3.783616304397583, 3.4143054485321045, 2.7572174072265625, 3.9879701137542725, 4.896751880645752, 3.5406088829040527, 3.014012575149536, 4.1022515296936035, 4.363819122314453, 3.268242835998535, 3.3484208583831787, 3.695322275161743, 3.441802501678467, 2.2730307579040527, 3.170745849609375, 3.2593486309051514, 2.4726901054382324, 2.627046585083008, 3.1244773864746094, 3.0629100799560547, 3.9747016429901123, 2.6889166831970215, 3.4328651428222656, 3.648609161376953, 3.696404457092285, 3.72829008102417, 2.5635828971862793, 3.129427194595337, 4.040737152099609, 4.7511796951293945, 3.3793840408325195, 3.515852212905884, 3.066004514694214, 2.70599365234375, 3.2258846759796143, 3.676652431488037, 3.3219738006591797, 4.182495594024658, 2.975252151489258, 4.13737678527832, 3.608562469482422, 3.7195963859558105, 5.316646099090576, 3.7008626461029053, 2.8783557415008545, 3.117354393005371, 3.1233177185058594, 3.05486798286438, 3.94372820854187, 3.143728256225586, 3.736766815185547, 2.739992380142212, 4.443780422210693, 4.089937686920166, 3.120366334915161, 3.2444827556610107, 2.7024996280670166, 4.671102046966553, 5.280534267425537, 4.0142436027526855, 6.878107070922852, 5.0715765953063965, 4.241385459899902, 6.065014839172363, 5.83128023147583, 3.936418294906616, 3.1781182289123535, 6.391221523284912, 9.776400566101074, 4.622241020202637]

![image](https://user-images.githubusercontent.com/27129645/220721659-a9999c1e-6f90-4ee4-a56f-3d440d1b14a5.png)

The aim is to find the maximum learning rate where the loss suddenly spikes up. From this plot that value will be around 0.08.

This will be the MaxLR for the One Cycle Policy. The learning rate will be linearly increased for a few epochs till the MaxLR, and will linearly decrease after that.

The following training logs show the LRs for each epoch.

    Epoch=0 LR=0.010507004006262685 Loss=1.819459739853354 batch_id=390 Accuracy=39.39%: 100%|██████████| 391/391 [00:30<00:00, 12.67it/s]

    Test set: Average loss: 1.6975, Accuracy: 4723/10000 (47.23%)

    Epoch=1 LR=0.029705862949006734 Loss=1.6555639874294896 batch_id=390 Accuracy=50.34%: 100%|██████████| 391/391 [00:30<00:00, 12.72it/s]

    Test set: Average loss: 1.3672, Accuracy: 5982/10000 (59.82%)

    Epoch=2 LR=0.05345378313174099 Loss=1.183008669434911 batch_id=390 Accuracy=62.22%: 100%|██████████| 391/391 [00:29<00:00, 13.25it/s]

    Test set: Average loss: 0.7989, Accuracy: 7162/10000 (71.62%)

    Epoch=3 LR=0.07266807593481178 Loss=0.8358164989125089 batch_id=390 Accuracy=71.54%: 100%|██████████| 391/391 [00:29<00:00, 13.35it/s]

    Test set: Average loss: 0.7464, Accuracy: 7493/10000 (74.93%)

    Epoch=4 LR=0.07999999965008367 Loss=0.7123448339569599 batch_id=390 Accuracy=75.27%: 100%|██████████| 391/391 [00:29<00:00, 13.22it/s]

    Test set: Average loss: 0.6051, Accuracy: 7918/10000 (79.18%)

    Epoch=5 LR=0.0794536289957299 Loss=0.6290334601841314 batch_id=390 Accuracy=77.84%: 100%|██████████| 391/391 [00:30<00:00, 12.70it/s]

    Test set: Average loss: 0.4747, Accuracy: 8418/10000 (84.18%)

    Epoch=6 LR=0.07783116096876898 Loss=0.5638376880637215 batch_id=390 Accuracy=80.37%: 100%|██████████| 391/391 [00:30<00:00, 13.00it/s]

    Test set: Average loss: 0.4792, Accuracy: 8389/10000 (83.89%)

    Epoch=7 LR=0.07517684854936703 Loss=0.5238776698594203 batch_id=390 Accuracy=81.62%: 100%|██████████| 391/391 [00:29<00:00, 13.27it/s]

    Test set: Average loss: 0.4954, Accuracy: 8348/10000 (83.48%)

    Epoch=8 LR=0.07156308837825812 Loss=0.48763953435146595 batch_id=390 Accuracy=82.90%: 100%|██████████| 391/391 [00:29<00:00, 13.27it/s]

    Test set: Average loss: 0.4423, Accuracy: 8526/10000 (85.26%)

    Epoch=9 LR=0.06708844613101779 Loss=0.45697480653557937 batch_id=390 Accuracy=84.06%: 100%|██████████| 391/391 [00:29<00:00, 13.24it/s]

    Test set: Average loss: 0.5303, Accuracy: 8300/10000 (83.00%)

    Epoch=10 LR=0.061874968129254256 Loss=0.4320530585773156 batch_id=390 Accuracy=84.88%: 100%|██████████| 391/391 [00:29<00:00, 13.17it/s]

    Test set: Average loss: 0.4485, Accuracy: 8552/10000 (85.52%)

    Epoch=11 LR=0.056064852514795024 Loss=0.4054454220911426 batch_id=390 Accuracy=85.91%: 100%|██████████| 391/391 [00:30<00:00, 12.97it/s]

    Test set: Average loss: 0.4496, Accuracy: 8529/10000 (85.29%)

    Epoch=12 LR=0.04981657078093111 Loss=0.37458182471182644 batch_id=390 Accuracy=86.89%: 100%|██████████| 391/391 [00:29<00:00, 13.08it/s]

    Test set: Average loss: 0.3816, Accuracy: 8677/10000 (86.77%)

    Epoch=13 LR=0.04330054544634768 Loss=0.35505028019475815 batch_id=390 Accuracy=87.51%: 100%|██████████| 391/391 [00:31<00:00, 12.37it/s]

    Test set: Average loss: 0.4151, Accuracy: 8671/10000 (86.71%)

    Epoch=14 LR=0.03669450176362381 Loss=0.3342376458065589 batch_id=390 Accuracy=88.20%: 100%|██████████| 391/391 [00:29<00:00, 13.24it/s]

    Test set: Average loss: 0.3713, Accuracy: 8778/10000 (87.78%)

    Epoch=15 LR=0.030178620244923748 Loss=0.3012200141578074 batch_id=390 Accuracy=89.59%: 100%|██████████| 391/391 [00:30<00:00, 12.83it/s]

    Test set: Average loss: 0.3812, Accuracy: 8796/10000 (87.96%)

    Epoch=16 LR=0.02393062222023377 Loss=0.2725665496895685 batch_id=390 Accuracy=90.62%: 100%|██████████| 391/391 [00:29<00:00, 13.26it/s]

    Test set: Average loss: 0.3407, Accuracy: 8895/10000 (88.95%)

    Epoch=17 LR=0.01812092247004251 Loss=0.24083956306242882 batch_id=390 Accuracy=91.58%: 100%|██████████| 391/391 [00:30<00:00, 12.75it/s]

    Test set: Average loss: 0.3074, Accuracy: 9012/10000 (90.12%)

    Epoch=18 LR=0.012907981144900835 Loss=0.20753396412981745 batch_id=390 Accuracy=92.75%: 100%|██████████| 391/391 [00:29<00:00, 13.06it/s]

    Test set: Average loss: 0.3078, Accuracy: 9011/10000 (90.11%)

    Epoch=19 LR=0.00843398174872694 Loss=0.17517526564009658 batch_id=390 Accuracy=93.97%: 100%|██████████| 391/391 [00:30<00:00, 12.70it/s]

    Test set: Average loss: 0.2648, Accuracy: 9110/10000 (91.10%)

    Epoch=20 LR=0.004820953069300903 Loss=0.13867633685926953 batch_id=390 Accuracy=95.22%: 100%|██████████| 391/391 [00:29<00:00, 13.24it/s]

    Test set: Average loss: 0.2580, Accuracy: 9175/10000 (91.75%)

    Epoch=21 LR=0.0021674408306883515 Loss=0.11322854516451317 batch_id=390 Accuracy=96.24%: 100%|██████████| 391/391 [00:30<00:00, 12.83it/s]

    Test set: Average loss: 0.2481, Accuracy: 9202/10000 (92.02%)

    Epoch=22 LR=0.0005458198486110825 Loss=0.10010084427912216 batch_id=390 Accuracy=96.65%: 100%|██████████| 391/391 [00:31<00:00, 12.60it/s]

    Test set: Average loss: 0.2399, Accuracy: 9208/10000 (92.08%)

    Epoch=23 LR=3.2e-07 Loss=0.09017461662173576 batch_id=390 Accuracy=97.06%: 100%|██████████| 391/391 [00:29<00:00, 13.09it/s]

    Test set: Average loss: 0.2372, Accuracy: 9210/10000 (92.10%)

That gave us a training accuracy of 97.06% and a test accuracy of 92.10%. The LR can be observed to increase till the 5th epoch and decrease thereon after.
