{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Libs\n",
    "# =============================================================================\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from os.path import exists\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import math\n",
    "import re\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Transformer\n",
    "# =============================================================================\n",
    "def attention(q, k, v, mask = None, dropout = None):\n",
    "    scores = q.matmul(k.transpose(-2, -1))\n",
    "    scores /= math.sqrt(q.shape[-1])\n",
    "    \n",
    "    #mask\n",
    "    scores = scores if mask is None else scores.masked_fill(mask == 0, -1e3)\n",
    "    \n",
    "    scores = F.softmax(scores, dim = -1)\n",
    "    scores = dropout(scores) if dropout is not None else scores\n",
    "    output = scores.matmul(v)\n",
    "    return output\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, out_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "#        self.q_linear = nn.Linear(out_dim, out_dim)\n",
    "#        self.k_linear = nn.Linear(out_dim, out_dim)\n",
    "#        self.v_linear = nn.Linear(out_dim, out_dim)\n",
    "        self.linear = nn.Linear(out_dim, out_dim*3)\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.out_dim = out_dim\n",
    "        self.out_dim_per_head = out_dim // n_heads\n",
    "        self.out = nn.Linear(out_dim, out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def split_heads(self, t):\n",
    "        return t.reshape(t.shape[0], -1, self.n_heads, self.out_dim_per_head)\n",
    "    \n",
    "    def forward(self, x, y=None, mask=None):\n",
    "        #in decoder, y comes from encoder. In encoder, y=x\n",
    "        y = x if y is None else y\n",
    "        \n",
    "        qkv = self.linear(x) # BS * SEQ_LEN * (3*EMBED_SIZE_L)\n",
    "        q = qkv[:, :, :self.out_dim] # BS * SEQ_LEN * EMBED_SIZE_L\n",
    "        k = qkv[:, :, self.out_dim:self.out_dim*2] # BS * SEQ_LEN * EMBED_SIZE_L\n",
    "        v = qkv[:, :, self.out_dim*2:] # BS * SEQ_LEN * EMBED_SIZE_L\n",
    "        \n",
    "        #break into n_heads\n",
    "        q, k, v = [self.split_heads(t) for t in (q,k,v)]  # BS * SEQ_LEN * HEAD * EMBED_SIZE_P_HEAD\n",
    "        q, k, v = [t.transpose(1,2) for t in (q,k,v)]  # BS * HEAD * SEQ_LEN * EMBED_SIZE_P_HEAD\n",
    "        \n",
    "        #n_heads => attention => merge the heads => mix information\n",
    "        scores = attention(q, k, v, mask, self.dropout) # BS * HEAD * SEQ_LEN * EMBED_SIZE_P_HEAD\n",
    "        scores = scores.transpose(1,2).contiguous().view(scores.shape[0], -1, self.out_dim) # BS * SEQ_LEN * EMBED_SIZE_L\n",
    "        out = self.out(scores)  # BS * SEQ_LEN * EMBED_SIZE\n",
    "        \n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, inp_dim, inner_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(inp_dim, inner_dim)\n",
    "        self.linear2 = nn.Linear(inner_dim, inp_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #inp => inner => relu => dropout => inner => inp\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x)))) \n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, n_heads, inner_transformer_size, inner_ff_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(n_heads, inner_transformer_size, dropout)\n",
    "        self.ff = FeedForward(inner_transformer_size, inner_ff_size, dropout)\n",
    "        self.norm1 = nn.LayerNorm(inner_transformer_size)\n",
    "        self.norm2 = nn.LayerNorm(inner_transformer_size)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        x2 = self.norm1(x)\n",
    "        x = x + self.dropout1(self.mha(x2, mask=mask))\n",
    "        x2 = self.norm2(x)\n",
    "        x = x + self.dropout2(self.ff(x2))\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, n_code, n_heads, embed_size, inner_ff_size, n_embeddings, seq_len, dropout=.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        #model input\n",
    "        self.embeddings = nn.Embedding(n_embeddings, embed_size)\n",
    "        self.pe = PositionalEmbedding(embed_size, seq_len)\n",
    "        \n",
    "        #backbone\n",
    "        encoders = []\n",
    "        for i in range(n_code):\n",
    "            encoders += [EncoderLayer(n_heads, embed_size, inner_ff_size, dropout)]\n",
    "        self.encoders = nn.ModuleList(encoders)\n",
    "        \n",
    "        #language model\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.linear = nn.Linear(embed_size, n_embeddings, bias=False)\n",
    "                \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = x + self.pe(x)\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "# Positional Embedding\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = 80):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        pe.requires_grad = False\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.pe[:,:x.size(1)] #x.size(1) = seq_len\n",
    "    \n",
    "# =============================================================================\n",
    "# Dataset\n",
    "# =============================================================================\n",
    "class SentencesDataset(Dataset):\n",
    "    #Init dataset\n",
    "    def __init__(self, sentences, vocab, seq_len):\n",
    "        dataset = self\n",
    "        \n",
    "        dataset.sentences = sentences\n",
    "        dataset.vocab = vocab + ['<ignore>', '<oov>', '<mask>']\n",
    "        dataset.vocab = {e:i for i, e in enumerate(dataset.vocab)} \n",
    "        dataset.rvocab = {v:k for k,v in dataset.vocab.items()}\n",
    "        dataset.seq_len = seq_len\n",
    "        \n",
    "        #special tags\n",
    "        dataset.IGNORE_IDX = dataset.vocab['<ignore>'] #replacement tag for tokens to ignore\n",
    "        dataset.OUT_OF_VOCAB_IDX = dataset.vocab['<oov>'] #replacement tag for unknown words\n",
    "        dataset.MASK_IDX = dataset.vocab['<mask>'] #replacement tag for the masked word prediction task\n",
    "    \n",
    "    \n",
    "    #fetch data\n",
    "    def __getitem__(self, index, p_random_mask=0.15):\n",
    "        dataset = self\n",
    "        \n",
    "        #while we don't have enough word to fill the sentence for a batch\n",
    "        s = []\n",
    "        while len(s) < dataset.seq_len:\n",
    "            s.extend(dataset.get_sentence_idx(index % len(dataset)))\n",
    "            index += 1\n",
    "        \n",
    "        #ensure that the sequence is of length seq_len\n",
    "        s = s[:dataset.seq_len]\n",
    "        [s.append(dataset.IGNORE_IDX) for i in range(dataset.seq_len - len(s))] #PAD ok\n",
    "        \n",
    "        #apply random mask\n",
    "        #\n",
    "        s = [(random.randrange(0, len(dataset.vocab.items()) - 3), w) if random.random() < p_random_mask else (w, dataset.IGNORE_IDX) for w in s]\n",
    "        \n",
    "        return {'input': torch.Tensor([w[0] for w in s]).long(),\n",
    "                'target': torch.Tensor([w[1] for w in s]).long()}\n",
    "\n",
    "    #return length\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    #get words id\n",
    "    def get_sentence_idx(self, index):\n",
    "        dataset = self\n",
    "        s = dataset.sentences[index]\n",
    "        s = [dataset.vocab[w] if w in dataset.vocab else dataset.OUT_OF_VOCAB_IDX for w in s] \n",
    "        return s\n",
    "\n",
    "# =============================================================================\n",
    "# Methods / Class\n",
    "# =============================================================================\n",
    "def get_batch(loader, loader_iter):\n",
    "    try:\n",
    "        batch = next(loader_iter)\n",
    "    except StopIteration:\n",
    "        loader_iter = iter(loader)\n",
    "        batch = next(loader_iter)\n",
    "    return batch, loader_iter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing..\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# #Init\n",
    "# =============================================================================\n",
    "print('initializing..')\n",
    "batch_size = 256\n",
    "seq_len = 20\n",
    "embed_size = 256\n",
    "inner_ff_size = embed_size * 4\n",
    "n_heads = 8\n",
    "n_code = 8\n",
    "n_vocab = 40000\n",
    "dropout = 0.01\n",
    "# n_workers = 12\n",
    "\n",
    "#optimizer\n",
    "optim_kwargs = {'lr':1e-4, 'weight_decay':1e-4, 'betas':(.9,.999)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading text...\n",
      "tokenizing sentences...\n",
      "creating/loading vocab...\n",
      "creating dataset...\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Input\n",
    "# =============================================================================\n",
    "#1) load text\n",
    "print('loading text...')\n",
    "pth = 'askreddit.txt'\n",
    "sentences = open(pth).read().lower().split('\\n')\n",
    "\n",
    "#2) tokenize sentences (can be done during training, you can also use spacy udpipe)\n",
    "print('tokenizing sentences...')\n",
    "special_chars = ',?;.:/*!+-()[]{}\"\\'&'\n",
    "sentences = [re.sub(f'[{re.escape(special_chars)}]', ' \\g<0> ', s).split(' ') for s in sentences]\n",
    "sentences = [[w for w in s if len(w)] for s in sentences]\n",
    "\n",
    "#3) create vocab if not already created\n",
    "print('creating/loading vocab...')\n",
    "pth = 'vocab.txt'\n",
    "if not exists(pth):\n",
    "    words = [w for s in sentences for w in s]\n",
    "    vocab = Counter(words).most_common(n_vocab) #keep the N most frequent words\n",
    "    vocab = [w[0] for w in vocab]\n",
    "    open(pth, 'w+').write('\\n'.join(vocab))\n",
    "else:\n",
    "    vocab = open(pth).read().split('\\n')\n",
    "\n",
    "#4) create dataset\n",
    "print('creating dataset...')\n",
    "dataset = SentencesDataset(sentences, vocab, seq_len)\n",
    "# kwargs = {'num_workers':n_workers, 'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
    "kwargs = {'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
    "data_loader = torch.utils.data.DataLoader(dataset, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    3   163  1518    36     4    66    19   106 40001  1095    19    14\n",
      "    86     1   322   220     8    43    64    20]\n",
      "['i', 'relapsing', 'confused', 'as', 'to', 'how', 'pillows', 'records', '<oov>', 'upvotes', 'this', 'is', 'now', 'the', 'top', 'ruled', 'of', 'all', 'time', 'on']\n",
      "40000\n"
     ]
    }
   ],
   "source": [
    "print(dataset[300]['input'].numpy())\n",
    "s = [dataset.rvocab[data] for data in (dataset[300]['input'].numpy())]\n",
    "print(s)\n",
    "print(dataset.vocab['<ignore>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I am confused as to how this got 170k upvotes \n",
    "This is now the top post of all time on this subreddit...Nice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing model...\n",
      "initializing optimizer and loss...\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Model\n",
    "# =============================================================================\n",
    "#init model\n",
    "print('initializing model...')\n",
    "model = Transformer(n_code, n_heads, embed_size, inner_ff_size, len(dataset.vocab), seq_len, dropout)\n",
    "model = model.cuda()\n",
    "\n",
    "# =============================================================================\n",
    "# Optimizer\n",
    "# =============================================================================\n",
    "print('initializing optimizer and loss...')\n",
    "optimizer = optim.Adam(model.parameters(), **optim_kwargs)\n",
    "loss_model = nn.CrossEntropyLoss(ignore_index=dataset.IGNORE_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "it: 0  | loss 10.75  | Δw: 7.501\n",
      "it: 100  | loss 7.38  | Δw: 0.946\n",
      "it: 200  | loss 6.75  | Δw: 0.754\n",
      "it: 300  | loss 6.5  | Δw: 0.779\n",
      "it: 400  | loss 6.68  | Δw: 0.943\n",
      "it: 500  | loss 6.63  | Δw: 1.235\n",
      "it: 600  | loss 6.54  | Δw: 1.533\n",
      "it: 700  | loss 6.68  | Δw: 1.779\n",
      "it: 800  | loss 6.67  | Δw: 2.09\n",
      "it: 900  | loss 6.59  | Δw: 2.386\n",
      "it: 1000  | loss 6.53  | Δw: 2.667\n",
      "it: 1100  | loss 6.42  | Δw: 2.838\n",
      "it: 1200  | loss 6.42  | Δw: 3.41\n",
      "it: 1300  | loss 6.39  | Δw: 4.072\n",
      "it: 1400  | loss 6.54  | Δw: 4.063\n",
      "it: 1500  | loss 6.37  | Δw: 4.346\n",
      "it: 1600  | loss 6.35  | Δw: 4.905\n",
      "it: 1700  | loss 6.36  | Δw: 5.343\n",
      "it: 1800  | loss 6.26  | Δw: 5.991\n",
      "it: 1900  | loss 6.4  | Δw: 6.079\n",
      "it: 2000  | loss 6.25  | Δw: 6.336\n",
      "it: 2100  | loss 5.99  | Δw: 6.387\n",
      "it: 2200  | loss 6.14  | Δw: 6.985\n",
      "it: 2300  | loss 6.11  | Δw: 7.723\n",
      "it: 2400  | loss 5.86  | Δw: 7.827\n",
      "it: 2500  | loss 6.25  | Δw: 8.078\n",
      "it: 2600  | loss 6.06  | Δw: 8.37\n",
      "it: 2700  | loss 5.97  | Δw: 8.691\n",
      "it: 2800  | loss 6.0  | Δw: 9.034\n",
      "it: 2900  | loss 6.07  | Δw: 9.174\n",
      "it: 3000  | loss 5.95  | Δw: 9.499\n",
      "it: 3100  | loss 5.89  | Δw: 9.801\n",
      "it: 3200  | loss 6.17  | Δw: 10.807\n",
      "it: 3300  | loss 5.78  | Δw: 10.519\n",
      "it: 3400  | loss 5.89  | Δw: 10.465\n",
      "it: 3500  | loss 5.86  | Δw: 10.871\n",
      "it: 3600  | loss 5.63  | Δw: 11.491\n",
      "it: 3700  | loss 5.76  | Δw: 11.689\n",
      "it: 3800  | loss 5.85  | Δw: 11.428\n",
      "it: 3900  | loss 5.58  | Δw: 12.268\n",
      "it: 4000  | loss 5.6  | Δw: 12.152\n",
      "it: 4100  | loss 5.84  | Δw: 12.253\n",
      "it: 4200  | loss 5.78  | Δw: 12.699\n",
      "it: 4300  | loss 5.48  | Δw: 13.062\n",
      "it: 4400  | loss 5.49  | Δw: 12.904\n",
      "it: 4500  | loss 5.63  | Δw: 13.362\n",
      "it: 4600  | loss 5.54  | Δw: 13.732\n",
      "it: 4700  | loss 5.6  | Δw: 14.114\n",
      "it: 4800  | loss 5.67  | Δw: 12.952\n",
      "it: 4900  | loss 5.58  | Δw: 14.105\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Train\n",
    "# =============================================================================\n",
    "print('training...')\n",
    "print_each = 100\n",
    "model.train()\n",
    "batch_iter = iter(data_loader)\n",
    "n_iteration = 5000\n",
    "for it in range(n_iteration):\n",
    "    \n",
    "    #get batch\n",
    "    batch, batch_iter = get_batch(data_loader, batch_iter)\n",
    "    \n",
    "    #infer\n",
    "    masked_input = batch['input']\n",
    "    masked_target = batch['target']\n",
    "    #print(masked_input)\n",
    "    #print(masked_target)\n",
    "    #print(masked_target.view(-1,1).squeeze())\n",
    "    \n",
    "    masked_input = masked_input.cuda(non_blocking=True)\n",
    "    masked_target = masked_target.cuda(non_blocking=True)\n",
    "    output = model(masked_input)\n",
    "    \n",
    "    #compute the cross entropy loss \n",
    "    output_v = output.view(-1,output.shape[-1])\n",
    "    target_v = masked_target.view(-1,1).squeeze()\n",
    "    loss = loss_model(output_v, target_v)\n",
    "    \n",
    "    #compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    #apply gradients\n",
    "    optimizer.step()\n",
    "    \n",
    "    #print step\n",
    "    if it % print_each == 0:\n",
    "        print('it:', it, \n",
    "              ' | loss', np.round(loss.item(),2),\n",
    "              ' | Δw:', round(model.embeddings.weight.grad.abs().sum().item(),3))\n",
    "    \n",
    "    #reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving embeddings...\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Results analysis\n",
    "# =============================================================================\n",
    "print('saving embeddings...')\n",
    "N = 3000\n",
    "np.savetxt('values.tsv', np.round(model.embeddings.weight.detach().cpu().numpy()[0:N], 2), delimiter='\\t', fmt='%1.2f')\n",
    "s = [dataset.rvocab[i] for i in range(N)]\n",
    "open('names.tsv', 'w+').write('\\n'.join(s) )\n",
    "\n",
    "\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'was', 'walking', 'down', 'the', 'swimming', 'when', 'i', 'saw', 'a', 'wild', 'fox', 'in', 'the', 'dense', 'forest', 'running', 'with', 'her', 'cubs']\n",
      "tensor([    3,    16,   754,   127,     1,  6255,    55,     3,   337,     6,\n",
      "         1415,  4363,    11,     1,  7097,  1838,   662,    21,    70, 15153],\n",
      "       device='cuda:0')\n",
      "256\n",
      "tensor([[ 4.2942,  5.9770,  4.2406,  ...,  3.9193,  5.0264,  7.0111],\n",
      "        [ 2.6048,  2.9704,  4.5942,  ...,  4.0873,  5.0279,  3.5757],\n",
      "        [ 2.5560,  3.0213,  3.2174,  ...,  4.2738,  2.4897,  7.8564],\n",
      "        ...,\n",
      "        [-2.5417, -2.5413, -2.5831,  ..., -2.6271, -2.5437, -2.5509],\n",
      "        [-2.5429, -2.3528, -2.5372,  ..., -2.5226, -2.5376, -2.5443],\n",
      "        [-2.5462, -2.5381, -2.5626,  ..., -2.5450,  4.7651, -2.5423]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "torch.Size([40003, 20])\n",
      "tensor([16002, 24002, 34003,  8001, 14001, 14002, 16004, 24002,     3, 14004,\n",
      "         6001,  6000, 16002,  6000, 14002, 36002, 24001,  6000, 12000, 14002],\n",
      "       device='cuda:0')\n",
      "torch.Size([20])\n",
      "[[ 4.29423    5.9769597  4.240621  ...  3.9192595  5.026415   7.0110846]\n",
      " [ 2.6048145  2.9704208  4.59418   ...  4.0872526  5.0278587  3.5757015]\n",
      " [ 2.5559754  3.0213428  3.2174473 ...  4.2737856  2.4897358  7.856371 ]\n",
      " ...\n",
      " [-2.5417333 -2.541295  -2.583084  ... -2.6271207 -2.5436757 -2.5508683]\n",
      " [-2.5429032 -2.3528204 -2.5371895 ... -2.5226157 -2.5375693 -2.5442758]\n",
      " [-2.5461774 -2.5380754 -2.5626419 ... -2.545005   4.765148  -2.542273 ]]\n",
      "['waffles', 'basicly', 'sidenote', 'rot', 'sentimental', 'baconreader', 'shiver', 'basicly', 'i', 'brow', 'moderate', 'claus', 'waffles', 'claus', 'baconreader', 'denier', 'stunningly', 'claus', 'axis', 'baconreader']\n"
     ]
    }
   ],
   "source": [
    "masked_input= ['i', 'was', 'walking', 'down', 'the', 'swimming', 'when', 'i', 'saw', 'a', 'wild', 'fox', 'in', 'the', 'dense', 'forest', 'running', 'with', 'her','cubs']\n",
    "#ensure that the sequence is of length seq_len\n",
    "masked_input = masked_input[:dataset.seq_len]\n",
    "[masked_input.append(dataset.IGNORE_IDX) for i in range(dataset.seq_len - len(masked_input))] #PAD ok\n",
    "masked_input = torch.Tensor([dataset.vocab[w] if w in dataset.vocab else dataset.OUT_OF_VOCAB_IDX for w in masked_input]).long().cuda()\n",
    "print([dataset.rvocab[data.item()] for data in masked_input])\n",
    "\n",
    "print(masked_input)\n",
    "print(output.shape[0])\n",
    "output = model(masked_input).view(-1, dataset.seq_len)\n",
    "print(output)\n",
    "print(output.size())\n",
    "print(output.argmax(0))\n",
    "print(output.argmax(0).size())\n",
    "print(output.detach().cpu().numpy())\n",
    "output_str = [dataset.rvocab[data] for data in (output.argmax(0).detach().cpu().numpy())]\n",
    "print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b8fbfcbe0e544000e4ba3d2d9974592a7ba1a2af52205db5302ae41a0c45d995"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
