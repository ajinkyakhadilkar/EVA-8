{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Kq1_e5Sx0HO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e43a699b-97c7-4ae9-9d9f-7436c99a5142"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.0+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.14.0+cu116)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: torch==1.13.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.13.0+cu116)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchvision) (4.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.21.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install numpy\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision # provide access to datasets, models, transforms, utils, etc\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "metadata": {
        "id": "SwQrgEUslre9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MNIST Dataset\n",
        "train_set = torchvision.datasets.MNIST(\n",
        "    root='./data'\n",
        "    ,train=True\n",
        "    ,download=True\n",
        "    ,transform=transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        ")\n",
        "print(train_set[0][0].size)\n",
        "train_loader = torch.utils.data.DataLoader(train_set\n",
        "    ,batch_size=32\n",
        "    ,shuffle=True\n",
        ")\n",
        "\n",
        "torch.set_printoptions(linewidth=120)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "H8p-yEQ2npQF",
        "outputId": "3125e7fb-c48b-4cde-f445-ac71943fa79d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<built-in method size of Tensor object at 0x7f5f3a246b80>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-9218f9ebaf11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m train_loader = torch.utils.data.DataLoader(train_set\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute 'to'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_num_correct(preds, labels):\n",
        "  return preds.argmax(dim=1).eq(labels).sum().item()\n"
      ],
      "metadata": {
        "id": "2GE59JnVntME"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    #conv1 | input=28*28*1 | output=24*24*6 | RF=5\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
        "    #MP1 | input=24*24*6 | output=12*12*6 | RF=10\n",
        "    #conv2 | input=12*12*6 | output=8*8*12 | RF=14\n",
        "    self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5) \n",
        "    #conv3 | input=8*8*12 | output=4*4*32 | RF=18\n",
        "    self.conv3 = nn.Conv2d(in_channels=12, out_channels=32, kernel_size=5)\n",
        "    #MP2 | input=4*4*32 | output=2*2*32 | RF=36\n",
        "    #FC1 | input=2*2*32 | out_features = 32\n",
        "    self.fc1 = nn.Linear(in_features=32*2*2, out_features=32)\n",
        "    #Output/MNIST | input=32 | out_features=10\n",
        "    self.out = nn.Linear(in_features=32, out_features=10)\n",
        "    #FC2/Sum | input=11 | out_features=19\n",
        "    self.fc2 = nn.Linear(in_features=11, out_features=19)\n",
        "  \n",
        "  def forward(self, t, num, model):\n",
        "    x = t\n",
        "    if model == 'mnist':\n",
        "      # input layer\n",
        "      \n",
        "      # conv1 layer\n",
        "      x = self.conv1(x)\n",
        "      x = F.relu(x)\n",
        "      x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "      \n",
        "      # conv2 layer\n",
        "      x = self.conv2(x)\n",
        "      x = F.relu(x)\n",
        "      \n",
        "      #conv3 1d conv layer\n",
        "      x = self.conv3(x)\n",
        "      x = F.relu(x)\n",
        "      x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "      \n",
        "      # reshape\n",
        "      #print(x.shape)\n",
        "      x = x.reshape(-1, 32*2*2)\n",
        "      \n",
        "      # fc1 layer\n",
        "      x = self.fc1(x)\n",
        "      x = F.relu(x)\n",
        "      \n",
        "      # fc2 layer\n",
        "      #x = self.fc2(x)\n",
        "      #x = F.relu(x)\n",
        "      \n",
        "      # output layer\n",
        "      x = self.out(x)\n",
        "      #print(x.size())\n",
        "      #print(num.size())\n",
        "      #print(torch.transpose(num, 0, 1).size())\n",
        "    if model=='sum':\n",
        "      #merge prediction and random number\n",
        "      x=torch.cat((x, torch.transpose(num, 0, 1)), 1)\n",
        "      x = self.fc2(x)\n",
        "\n",
        "\n",
        "    #x = F.softmax(x, dim=1)\n",
        "    return x"
      ],
      "metadata": {
        "id": "TFPyQrOPnvhs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_random_num():\n",
        "  return torch.tensor([[random.randrange(1,10) for i in range(0,32)]])"
      ],
      "metadata": {
        "id": "2kyvthlqzJmD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network = Network()\n",
        "device = torch.device('cuda')\n",
        "\n",
        "torch.set_grad_enabled(True)\n",
        "\n",
        "for epoch in range(30):\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    lr = 1/((epoch+1)*(epoch+1)*(epoch+1))\n",
        "    optimizer = optim.Adam(network.parameters(), 0.001)\n",
        "\n",
        "    for batch in train_loader: # Get Batch\n",
        "\n",
        "        images, labels = batch\n",
        "        rand_num = get_random_num()\n",
        "\n",
        "        preds_mnist = network(images, rand_num, 'mnist') # Pass Batch\n",
        "        preds_sum = network(preds_mnist, rand_num, 'sum') # Pass Batch\n",
        "        #print(labels)\n",
        "        #print(rand_num[0])\n",
        "        loss_mnist = F.cross_entropy(preds_mnist, labels) # Calculate Loss\n",
        "        loss_sum = F.cross_entropy(preds_sum, torch.add(labels, rand_num[0])) # Calculate Loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss_sum.backward() # Calculate Gradients\n",
        "        optimizer.step() # Update Weights\n",
        "\n",
        "        total_loss += loss_sum.item()\n",
        "        total_correct += get_num_correct(preds_sum, torch.add(labels, rand_num[0]))\n",
        "        #print('loss after batch: '+str(total_loss))\n",
        "        #print('correct after batch: '+str(total_correct))\n",
        "\n",
        "    \"\"\"\n",
        "    preds = network(images) # Pass Batch\n",
        "    loss = F.cross_entropy(preds, labels) # Calculate Loss\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward() # Calculate Gradients\n",
        "    optimizer.step() # Update Weights\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    total_correct += get_num_correct(preds, labels)\n",
        "    \"\"\"\n",
        "    print(\n",
        "        \"epoch:\", epoch, \n",
        "        \"total_correct:\", total_correct, \n",
        "        \"loss:\", total_loss\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pt92CdubnynE",
        "outputId": "e3767ee6-8959-49b7-d4b3-46a6968b58c5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0 total_correct: 10390 loss: 4429.671415448189\n",
            "epoch: 1 total_correct: 18438 loss: 3460.265366077423\n",
            "epoch: 2 total_correct: 22444 loss: 3116.235142827034\n",
            "epoch: 3 total_correct: 25308 loss: 2914.4169557094574\n",
            "epoch: 4 total_correct: 27150 loss: 2776.4529504776\n",
            "epoch: 5 total_correct: 28924 loss: 2667.3346207141876\n",
            "epoch: 6 total_correct: 30617 loss: 2578.0346195697784\n",
            "epoch: 7 total_correct: 31800 loss: 2497.303243994713\n",
            "epoch: 8 total_correct: 33094 loss: 2432.204349040985\n",
            "epoch: 9 total_correct: 34154 loss: 2367.8478249311447\n",
            "epoch: 10 total_correct: 35203 loss: 2316.396575450897\n",
            "epoch: 11 total_correct: 36276 loss: 2266.422708451748\n",
            "epoch: 12 total_correct: 37505 loss: 2222.670917034149\n",
            "epoch: 13 total_correct: 38123 loss: 2174.9415504932404\n",
            "epoch: 14 total_correct: 38808 loss: 2137.7498277425766\n",
            "epoch: 15 total_correct: 39911 loss: 2104.5669313669205\n",
            "epoch: 16 total_correct: 40265 loss: 2068.424749970436\n",
            "epoch: 17 total_correct: 40849 loss: 2037.6919121742249\n",
            "epoch: 18 total_correct: 42122 loss: 2006.7010207176208\n",
            "epoch: 19 total_correct: 42278 loss: 1980.1332124471664\n",
            "epoch: 20 total_correct: 43565 loss: 1946.6533694267273\n",
            "epoch: 21 total_correct: 43923 loss: 1924.9403038024902\n",
            "epoch: 22 total_correct: 44083 loss: 1896.6454605460167\n",
            "epoch: 23 total_correct: 44444 loss: 1873.3748390078545\n",
            "epoch: 24 total_correct: 45137 loss: 1854.2574985027313\n",
            "epoch: 25 total_correct: 45494 loss: 1829.713345348835\n",
            "epoch: 26 total_correct: 45807 loss: 1813.2161071300507\n",
            "epoch: 27 total_correct: 46203 loss: 1789.4057226777077\n",
            "epoch: 28 total_correct: 46315 loss: 1770.0101960897446\n",
            "epoch: 29 total_correct: 47001 loss: 1750.6544769406319\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = torchvision.datasets.MNIST(\n",
        "    root='./data'\n",
        "    ,train=False\n",
        "    ,download=True\n",
        "    ,transform=transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_set\n",
        "    ,batch_size=1\n",
        "    ,shuffle=True\n",
        ")\n",
        "\n",
        "#print(next(iter(test_loader)))\n",
        "#image, label = next(iter(test_loader))\n",
        "#plt.imshow(image.squeeze(), cmap='gray')\n",
        "#print('label:', label)"
      ],
      "metadata": {
        "id": "Hn6-u1aMrSPd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = next(iter(test_loader))\n",
        "network(network(image, None, 'mnist'), 5, 'sum')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "zf6IrT8brY30",
        "outputId": "aa4a8622-9608-4d62-8e26-df12f2d5adcc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-e0f24f2bd2c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mnist'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-563bc3bef74a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, t, num, model)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m       \u001b[0;31m#merge prediction and random number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m       \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: transpose() received an invalid combination of arguments - got (int, int, int), but expected one of:\n * (Tensor input, int dim0, int dim1)\n      didn't match because some of the arguments have invalid types: (!int!, int, int)\n * (Tensor input, name dim0, name dim1)\n      didn't match because some of the arguments have invalid types: (!int!, !int!, !int!)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = next(iter(test_loader))\n",
        "plt.imshow(image.squeeze(), cmap='gray')\n",
        "print('label:', label)\n",
        "torch.argmax(network(network(image,None, 'mnist'), torch.tensor([[7]]), 'sum'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "aqMCjGYarisE",
        "outputId": "29205081-2f27-4fbd-a676-7323e22bdb12"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label: tensor([2])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(9)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}